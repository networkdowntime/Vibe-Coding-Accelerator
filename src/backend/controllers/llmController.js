const axios = require('axios');
const fs = require('fs').promises;
const path = require('path');
const { v4: uuidv4 } = require('uuid');

// In-memory storage for processing jobs (in production, use Redis or database)
const processingJobs = new Map();

// Job status constants
const JOB_STATUS = {
  PENDING: 'pending',
  PROCESSING: 'processing',
  COMPLETED: 'completed',
  FAILED: 'failed',
  CANCELLED: 'cancelled'
};

/**
 * Load OpenAPI settings from environment variables
 */
const getOpenAPISettings = () => {
  const endpoint = process.env.OPENAPI_ENDPOINT;
  const apiKey = process.env.OPENAPI_API_KEY;
  
  if (!endpoint) {
    throw new Error('OpenAPI endpoint must be configured');
  }
  
  return { endpoint, apiKey };
};

/**
 * Create HTTP client for LLM API calls
 */
const createLLMClient = () => {
  const { endpoint, apiKey } = getOpenAPISettings();
  
  // API key is optional; only add Authorization header if present
  const headers = {
    'Content-Type': 'application/json'
  };
  if (apiKey) {
    headers['Authorization'] = `Bearer ${apiKey}`;
  }
  return axios.create({
    baseURL: endpoint,
    headers: headers,
    timeout: 1200000 // 30 second timeout
  });
};

/**
 * Get project configuration from project-config.json
 */
const getProjectConfig = async (projectId) => {
  try {
    const configPath = path.join(process.cwd(), '..', '..', 'projects', projectId, 'project-config.json');
    const configContent = await fs.readFile(configPath, 'utf-8');
    return JSON.parse(configContent);
  } catch (error) {
    throw new Error(`Failed to read project configuration: ${error.message}`);
  }
};

/**
 * Get tech stack instruction files from AI agent directory
 */
const getTechStackFiles = async (aiAgent, techStackIds) => {
  try {
    const aiAgentPath = path.join(process.cwd(), '..', '..', 'ai_agents', aiAgent, 'instructions');
    const techStackFiles = [];
    
    for (const techStackId of techStackIds) {
      try {
        const filePath = path.join(aiAgentPath, techStackId);
        const content = await fs.readFile(filePath, 'utf-8');
        techStackFiles.push({
          id: techStackId,
          path: techStackId,
          content,
          size: content.length
        });
      } catch (error) {
        console.warn(`Warning: Could not read tech stack file ${techStackId}:`, error.message);
      }
    }
    
    return techStackFiles;
  } catch (error) {
    throw new Error(`Failed to get tech stack files: ${error.message}`);
  }
};

/**
 * Get list of project files to use as context
 */
const getProjectFiles = async (projectId) => {
  try {
    const projectPath = path.join(process.cwd(), '..', '..', 'projects', projectId, 'files');
    const files = [];
    
    // Recursively get all relevant files
    const scanDirectory = async (dirPath, relativePath = '') => {
      try {
        const entries = await fs.readdir(dirPath, { withFileTypes: true });
        
        for (const entry of entries) {
          const fullPath = path.join(dirPath, entry.name);
          const relPath = path.join(relativePath, entry.name);
          
          if (entry.isDirectory()) {
            // Skip node_modules, .git, and other common ignore patterns
            if (!['node_modules', '.git', 'dist', 'build', '.angular'].includes(entry.name)) {
              await scanDirectory(fullPath, relPath);
            }
          } else if (entry.isFile()) {
            // Include relevant file types
            const ext = path.extname(entry.name).toLowerCase();
            const relevantExtensions = ['.js', '.ts', '.json', '.md', '.yml', '.yaml', '.html', '.css', '.scss'];
            
            if (relevantExtensions.includes(ext)) {
              const content = await fs.readFile(fullPath, 'utf-8');
              files.push({
                path: relPath,
                content,
                size: content.length
              });
            }
          }
        }
      } catch (error) {
        console.warn(`Warning: Could not scan directory ${dirPath}:`, error.message);
      }
    };
    
    await scanDirectory(projectPath);
    return files;
  } catch (error) {
    throw new Error(`Failed to get project files: ${error.message}`);
  }
};


/**
 * Process a tech stack instruction file with project context
 */
const processTechStackWithLLM = async (techStackFile, processedProjectFilesWithLLM, llmClient) => {
  try {
    const developer_prompt = `You are an expert software architect tasked with aligning a technology-specific instruction file with a project's unified guidelines.`;
    
    const prompt = `You will be provided with:
1. A "Project Guidelines" Markdown file (generated by a prior merging process).
2. A "Tech Stack Instruction" Markdown file for a specific technology stack (includes general coding guidelines, commenting guidelines, security standards, etc.).

TASK
Compare the "Tech Stack Instruction" file against the "Project Guidelines" file and update the "Tech Stack Instruction" file file as follows:

1. Add any tech stack-specific guidelines from the Project Guidelines that are missing from the Tech Stack Instruction file.
2. Remove any tech stack-specific guidelines from the Tech Stack Instruction file that contradict the Project Guidelines.
3. Update any tech stack-specific guidelines in the Tech Stack Instruction file that need to be edited to conform with the Project Guidelines.
4. Preserve exact formatting, including ASCII diagrams, code blocks, and indentation.
5. Do not change the overall structure of the Tech Stack Instruction file or repeat content unless necessary to incorporate new guidelines.
6. Do not add any new guidelines that are not present in the Project Guidelines file.

The goal is to produce an updated Tech Stack Instruction Markdown file that:
- Incorporates relevant project-specific guidelines.
- Removes or modifies content that is contradictory or incompatible.
- Preserves all valid existing content from the original tech stack instruction file.


OUTPUT FORMAT
Return your result in a JSON + Markdown hybrid format as follows, DO NOT add any additional text outside of this structure:

{
  "updatedTechStackInstructions": "<<<BEGIN_MARKDOWN>>>\n{{COMPLETE_UPDATED_TECH_STACK_INSTRUCTION_MARKDOWN}}\n<<<END_MARKDOWN>>>",
  "appliedGuidelines": [
    {
      "id": "{{GUIDELINE_ID}}",
      "reason": "{{SHORT_DESCRIPTION_OF_HOW_THIS_GUIDELINE_WAS_APPLIED}}"
    }
    // ... additional applied guidelines
  ],
  "unappliedGuidelines": [
    {
      "id": "{{GUIDELINE_ID}}",
      "reason": "{{SHORT_DESCRIPTION_OF_WHY_THIS_GUIDELINE_WAS_NOT_APPLIED}}"
    }
    // ... additional unapplied guidelines
  ]
}

NOTES FOR LLM
- Do not output any text outside of the JSON structure.
- Always preserve the exact Markdown formatting of the updatedTechStackInstructions content, including multi-line formatting, indentation, and code blocks.
- The "appliedGuidelines" and "unappliedGuidelines" arrays should reference the exact guideline IDs from the Project Guidelines file (e.g., GDL-001).
- If a guideline is already present in the Tech Stack Instruction file, it should not be added again and added to the appliedGuidelines array with a reason of "Already present in Tech Stack Instruction file".
- Be explicit when documenting contradictions in the "reason" field for unapplied guidelines.
- Ensure that the updatedTechStackInstructions content is complete and can be saved directly as the new Tech Stack Instruction file.

PROJECT GUIDELINES MARKDOWN:
<file_content>
${processedProjectFilesWithLLM}
</file_content>

TECH STACK INSTRUCTION MARKDOWN TO UPDATE:
File: ${techStackFile.path}
<file_content>
${techStackFile.content}
</file_content>
`;

    const request = {
      model: 'gpt-4.1-mini',
      messages: [
        {
          role: 'system',
          content: developer_prompt
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      max_tokens: 10000,
      temperature: 0.1      
    }
    
    const response = await llmClient.post('/v1/chat/completions', request);
    
    if (!response.data?.choices?.[0]?.message?.content) {
      throw new Error('Invalid response from LLM API');
    }

    // Parse LLM response
    let llmContent = response.data.choices[0].message.content;
    
    llmContent = llmContent.replace(/<think>[\s\S]*?<\/think>/i, ''); // Remove <think> tags

    console.log(llmContent);

    // Try to extract JSON from the response
    let processedResult;
    try {
      // Look for JSON in the response
      const jsonMatch = llmContent.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        
        // Extract the markdown content from the new format
        let updatedContent = parsed.updatedTechStackInstructions;
        if (updatedContent && updatedContent.includes('<<<BEGIN_MARKDOWN>>>')) {
          const markdownMatch = updatedContent.match(/<<<BEGIN_MARKDOWN>>>\n([\s\S]*?)\n<<<END_MARKDOWN>>>/);
          if (markdownMatch) {
            updatedContent = markdownMatch[1];
          }
        }
        
        processedResult = {
          updatedContent: updatedContent || parsed.updatedTechStackInstructions,
          appliedGuidelines: parsed.appliedGuidelines || [],
          unappliedGuidelines: parsed.unappliedGuidelines || [],
          rawResponse: parsed
        };
      } else {
        // Fallback: create structured response from plain text
        processedResult = {
          updatedContent: llmContent,
          appliedGuidelines: [],
          unappliedGuidelines: [],
          rawResponse: null
        };
      }
    } catch (parseError) {
      // Fallback for non-JSON responses
      processedResult = {
        updatedContent: llmContent,
        appliedGuidelines: [],
        unappliedGuidelines: [],
        rawResponse: null
      };
    }

    return {
      originalFile: techStackFile,
      processedResult,
      processingTime: new Date().toISOString()
    };
  } catch (error) {
    if (error.response?.status === 429) {
      throw new Error('Rate limit exceeded. Please try again later.');
    } else if (error.response?.status === 401) {
      throw new Error('Invalid API key. Please check your OpenAPI configuration.');
    } else if (error.code === 'ECONNABORTED') {
      throw new Error('Request timeout. The LLM API took too long to respond.');
    } else {
      throw new Error(`LLM processing failed: ${error.message}`);
    }
  }
};

/**
 * Process the project context files to generate a unified set of guidelines that we can use for both the tech stack update and the traceability report.
 */
const processProjectFilesWithLLM = async (projectFiles, llmClient) => {
  try {
    const developer_prompt = `You are an expert software architect tasked with merging multiple project guideline documents into a single, cohesive, non-redundant set of guidelines.
You must preserve exact formatting (including ASCII diagrams, code blocks, and whitespace) and provide a traceability-friendly structured Markdown format for each guideline.`;

    const prompt = `INPUT DOCUMENTS
Below is the set of project guideline files to merge:
<file_content>
${projectFiles.map(file => `
--- ${file.path} ---
${file.content}
`).join('\n')}
</file_content>

TASK REQUIREMENTS
1. Merge All Guidelines
  - Remove duplicates.
  - Organize logically by category (Architecture, Coding Standards, Deployment, Security, etc.).
  - If two or more guidelines conflict, keep both but flag as Contradiction: Yes and note the conflicting guideline IDs.
  - How to split guidelines:
    - Don't split guidelines that specify a particular order of application, such as priority.
    - Do split guidelines that list multiple independent lists of items that can be applied in any order. ex. Versions, Modifiers, Patterns, etc.
  - Do not add any new guidelines that are not present in the input documents.
  - Do not add any descriptive notes or explanations outside of the guideline text.
2. Preserve Formatting
  - Retain multi-line formatting exactly.
  - Wrap all ASCII diagrams, directory structures, and code examples in Markdown fenced code blocks.
3. Assign Unique IDs
  - Use GDL-### format for each guideline (e.g., GDL-001).
  - IDs must be unique and sequential.
4. Track Source References
  - List all source filenames and line numbers where the guideline originated.
  - If line numbers are not available, reference at least the file name.
5. Output Format
  - Output only in the following Markdown structure:

OUTPUT FORMAT
# Unified Project Guidelines

> Generated by Vibe Coding Accelerator – Merged from {{NUMBER_OF_INPUT_FILES}} sources.
> Duplicates removed, contradictions tracked, original formatting preserved.

---

## Guideline 001
**ID:** GDL-001  
**Source(s):** {{SOURCE_FILE(S)_WITH_LINE_NUMBERS}}  
**Contradiction:** {{Yes/No}}  
**Category:** {{CATEGORY}}

{{GUIDELINE_TEXT}}
---

## Guideline 002
**ID:** GDL-002  
**Source(s):** {{SOURCE_FILE(S)_WITH_LINE_NUMBERS}}  
**Contradiction:** {{Yes/No – conflicts with GDL-###}}  
**Category:** {{CATEGORY}}

{{GUIDELINE_TEXT}}
---

NOTES FOR LLM
- Never rephrase guideline text unless absolutely necessary to resolve ambiguity; prefer original wording.
- Always preserve code indentation and spacing exactly.
- Never merge contradictory guidelines into one; instead, keep them separate and clearly flag contradictions.
- Return only the unified Markdown file; no explanations outside of the required structure.
`;

    const request = {
      model: 'gpt-4.1-mini',
      messages: [
        {
          role: 'system',
          content: developer_prompt
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      max_tokens: 10000,
      temperature: 0.1      
    }
    
    const response = await llmClient.post('/v1/chat/completions', request);

    if (!response.data?.choices?.[0]?.message?.content) {
      throw new Error('Invalid response from LLM API');
    }

    // Parse LLM response
    let unifiedProjectGuidelines = response.data.choices[0].message.content;
    
    // Remove <think></think> tags and their content as they break the markdown
    unifiedProjectGuidelines = unifiedProjectGuidelines.replace(/<think>[\s\S]*?<\/think>/i, '');

    // Remove any ```markdown ``` tags that might have been added by the LLM
    unifiedProjectGuidelines = unifiedProjectGuidelines.replace(/```markdown[\s\S]*?```/i, '');

    return unifiedProjectGuidelines;
  } catch (error) {
    if (error.response?.status === 429) {
      throw new Error('Rate limit exceeded. Please try again later.');
    } else if (error.response?.status === 401) {
      throw new Error('Invalid API key. Please check your OpenAPI configuration.');
    } else if (error.code === 'ECONNABORTED') {
      throw new Error('Request timeout. The LLM API took too long to respond.');
    } else {
      throw new Error(`LLM processing failed: ${error.message}`);
    }
  }
};

/**
 * Save processed tech stack files to export directory
 */
const saveProcessedTechStacks = async (projectId, jobId, processedTechStacks, unifiedProjectGuidelines) => {
  let exportDir;
  
  // Create export directory
  try {
    exportDir = path.join(process.cwd(), '..', '..', 'projects', projectId, 'exports', jobId);
    await fs.mkdir(exportDir, { recursive: true });
  } catch (error) {
    throw new Error(`Failed to create export directory: ${error.message}`);
  }
  
  // Save unified project guidelines
  try {
    const unifiedGuidelinesPath = path.join(exportDir, 'reports', 'unified-project-guidelines.md');
    await fs.mkdir(path.dirname(unifiedGuidelinesPath), { recursive: true });
    await fs.writeFile(unifiedGuidelinesPath, unifiedProjectGuidelines, 'utf-8');
  } catch (error) {
    throw new Error(`Failed to save unified project guidelines: ${error.message}`);
  }
  
  const results = [];
  const allAppliedGuidelines = new Map();
  const allUnappliedGuidelines = new Map();
  
    // Process each tech stack report file
  for (const processedTechStack of processedTechStacks) {
    const { originalFile, processedResult } = processedTechStack;

    // Save individual tech stack report
    try {
      const reportPath = path.join(exportDir, 'reports', `${originalFile.path}.json`);
      await fs.mkdir(path.dirname(reportPath), { recursive: true });
      const report = {
        techStackFile: originalFile.path,
        appliedGuidelines: processedResult.appliedGuidelines,
        unappliedGuidelines: processedResult.unappliedGuidelines,
        processingTime: processedTechStack.processingTime,
      };
      await fs.writeFile(reportPath, JSON.stringify(report, null, 2), 'utf-8');
    } catch (error) {
      console.error(`Failed to save tech stack report for ${originalFile.path}:`, error.message);
      throw new Error(`Failed to save tech stack report for ${originalFile.path}: ${error.message}`);
    }

    // Collect applied guidelines
    try {
      if (processedResult.appliedGuidelines && Array.isArray(processedResult.appliedGuidelines)) {
        processedResult.appliedGuidelines.forEach(guideline => {
          if (guideline.id) {
            allAppliedGuidelines.set(guideline.id, {
              id: guideline.id,
              reason: guideline.reason,
              appliedInFiles: allAppliedGuidelines.has(guideline.id) 
                ? [...allAppliedGuidelines.get(guideline.id).appliedInFiles, originalFile.path]
                : [originalFile.path]
            });
          }
        });
      }
    } catch (error) {
      console.error(`Failed to process applied guidelines for ${originalFile.path}:`, error.message);
      throw new Error(`Failed to process applied guidelines for ${originalFile.path}: ${error.message}`);
    }
    
    // Collect unapplied guidelines
    try {
      if (processedResult.unappliedGuidelines && Array.isArray(processedResult.unappliedGuidelines)) {
        processedResult.unappliedGuidelines.forEach(guideline => {
          if (guideline.id) {
            allUnappliedGuidelines.set(guideline.id, {
              id: guideline.id,
              reason: guideline.reason,
              unappliedInFiles: allUnappliedGuidelines.has(guideline.id)
                ? [...allUnappliedGuidelines.get(guideline.id).unappliedInFiles, originalFile.path]
                : [originalFile.path]
            });
          }
        });
      }
    } catch (error) {
      console.error(`Failed to process unapplied guidelines for ${originalFile.path}:`, error.message);
      throw new Error(`Failed to process unapplied guidelines for ${originalFile.path}: ${error.message}`);
    }

  }

  // Calculate project compliance
  let projectCompliance, totalAppliedGuidelines, totalUnappliedGuidelines, totalGuidelines;
  try {
    // Remove applied guidelines from unapplied guidelines
    for (const appliedId of allAppliedGuidelines.keys()) {
      allUnappliedGuidelines.delete(appliedId);
    }

    totalAppliedGuidelines = allAppliedGuidelines.size;
    totalUnappliedGuidelines = allUnappliedGuidelines.size;
    totalGuidelines = totalAppliedGuidelines + totalUnappliedGuidelines;
    projectCompliance = totalGuidelines > 0 ? Math.round((totalAppliedGuidelines / totalGuidelines) * 100) : 0;
  } catch (error) {
    throw new Error(`Failed to calculate project compliance: ${error.message}`);
  }
  
  // Process each tech stack file
  for (const processedTechStack of processedTechStacks) {
    const { originalFile, processedResult } = processedTechStack;
        
    // Save updated tech stack file
    try {
      const updatedPath = path.join(exportDir, '.github', 'instructions', originalFile.path);
      await fs.mkdir(path.dirname(updatedPath), { recursive: true });
      await fs.writeFile(updatedPath, processedResult.updatedContent, 'utf-8');
      
      results.push({
        updatedPath,
        reportPath: path.join(exportDir, 'reports', `${originalFile.path}.json`),
        fileName: originalFile.path,
        techStackId: originalFile.id
      });
    } catch (error) {
      console.error(`Failed to save updated tech stack file for ${originalFile.path}:`, error.message);
      throw new Error(`Failed to save updated tech stack file for ${originalFile.path}: ${error.message}`);
    }
  }

  // Save traceability report
  try {
    const traceabilityPath = path.join(exportDir, 'reports', 'traceability.json');
    const traceabilityReport = {
      jobId,
      completedAt: new Date().toISOString(),
      projectCompliance: `${projectCompliance}%`,
      totalTechStacks: processedTechStacks.length,
      totalGuidelines: totalGuidelines,
      totalAppliedGuidelines: totalAppliedGuidelines,
      totalUnappliedGuidelines: totalUnappliedGuidelines,
      appliedGuidelines: Array.from(allAppliedGuidelines.values()),
      unappliedGuidelines: Array.from(allUnappliedGuidelines.values())
    };
    await fs.writeFile(traceabilityPath, JSON.stringify(traceabilityReport, null, 2), 'utf-8');
  } catch (error) {
    throw new Error(`Failed to save traceability report: ${error.message}`);
  }
  
  // copy the README.md file, the chatmodes folder, and the prompts folder from the ai_agent/githubCopilot directory to the export directory
  try {
    const readmeSrc = path.join(process.cwd(), '..', '..', 'ai_agents', 'githubCopilot', 'README.md');
    const readmeDest = path.join(exportDir, 'README.md');
    await fs.copyFile(readmeSrc, readmeDest);

    const chatModesSrc = path.join(process.cwd(), '..', '..', 'ai_agents', 'githubCopilot', 'chatmodes');
    const chatModesDest = path.join(exportDir, '.github', 'chatmodes');
    await fs.mkdir(chatModesDest, { recursive: true });
    await fs.cp(chatModesSrc, chatModesDest, { recursive: true });

    const promptsSrc = path.join(process.cwd(), '..', '..', 'ai_agents', 'githubCopilot', 'prompts');
    const promptsDest = path.join(exportDir, '.github', 'prompts');
    await fs.mkdir(promptsDest, { recursive: true });
    await fs.cp(promptsSrc, promptsDest, { recursive: true });
  } catch (error) {
    console.error(`Failed to copy README.md file:`, error.message);
    throw new Error(`Failed to copy README.md file: ${error.message}`);
  }

  // create a zip file of the updated tech stack files
  try {
    const zip = require('adm-zip');
    const zipFilePath = path.join(exportDir, 'ai-agent-config.zip');
    const zipFile = new zip();
    zipFile.addLocalFolder(path.join(exportDir, '.github'), '.github');
    zipFile.addLocalFolder(path.join(exportDir, 'reports'), 'reports');
    zipFile.addLocalFile(path.join(exportDir, 'README.md'), '', 'README.md');
    await fs.writeFile(zipFilePath, zipFile.toBuffer());
  } catch (error) {
    console.error(`Failed to create zip file of updated tech stacks:`, error.message);
    throw new Error(`Failed to create zip file of updated tech stacks: ${error.message}`);
  }
  
  // Return the traceability report data
  const traceabilityReport = {
    jobId,
    completedAt: new Date().toISOString(),
    projectCompliance: `${projectCompliance}%`,
    totalTechStacks: processedTechStacks.length,
    totalGuidelines: totalGuidelines,
    totalAppliedGuidelines: totalAppliedGuidelines,
    totalUnappliedGuidelines: totalUnappliedGuidelines,
    appliedGuidelines: Array.from(allAppliedGuidelines.values()),
    unappliedGuidelines: Array.from(allUnappliedGuidelines.values())
  };
  
  return { exportDir, traceabilityReport };
};

/**
 * Start LLM processing for a project
 */
const startProcessing = async (req, res) => {
  try {
    const { projectId } = req.params;
    const jobId = uuidv4();
    
    // Validate project exists
    const projectPath = path.join(process.cwd(), '..', '..', 'projects', projectId);
    try {
      await fs.access(projectPath);
    } catch (error) {
      return res.status(404).json({ error: 'Project not found' });
    }
    
    // Initialize job
    const job = {
      id: jobId,
      projectId,
      status: JOB_STATUS.PENDING,
      createdAt: new Date().toISOString(),
      progress: {
        total: 0,
        completed: 0,
        failed: 0,
        current: null
      },
      files: [],
      processedFiles: [],
      errors: []
    };
    
    processingJobs.set(jobId, job);
    
    // Start processing asynchronously
    processProjectAsync(jobId);
    
    res.json({
      jobId,
      status: job.status,
      message: 'Processing started'
    });
  } catch (error) {
    console.error('Error starting LLM processing:', error);
    res.status(500).json({ error: error.message });
  }
};

/**
 * Asynchronous project processing - processes tech stack files with project context
 */
const processProjectAsync = async (jobId) => {
  const job = processingJobs.get(jobId);
  if (!job) return;
  
  try {
    // Update status to processing
    job.status = JOB_STATUS.PROCESSING;
    job.startedAt = new Date().toISOString();
    
    // Get project configuration
    const projectConfig = await getProjectConfig(job.projectId);
    const { 'tech-stack': techStackIds, 'ai-agent': aiAgent } = projectConfig;
    
    if (!techStackIds || !Array.isArray(techStackIds) || techStackIds.length === 0) {
      throw new Error('No tech stack files selected in project configuration');
    }
    
    if (!aiAgent) {
      throw new Error('No AI agent selected in project configuration');
    }
    
    // Get tech stack instruction files
    const techStackFiles = await getTechStackFiles(aiAgent, techStackIds);
    if (techStackFiles.length === 0) {
      throw new Error('No valid tech stack instruction files found');
    }
    
    // Get all project files for context
    const projectFiles = await getProjectFiles(job.projectId);
    
    job.techStackFiles = techStackFiles;
    job.projectFiles = projectFiles;
    job.progress.total = techStackFiles.length;
    
    // Create LLM client
    const llmClient = createLLMClient();
    
    // First, process all project files to create unified guidelines
    const processedProjectFilesWithLLM = await processProjectFilesWithLLM(projectFiles, llmClient);
    
    // Process each tech stack file with all project files as context
    for (let i = 0; i < techStackFiles.length; i++) {
      // Check if job was cancelled
      if (job.status === JOB_STATUS.CANCELLED) {
        return;
      }
      
      const techStackFile = techStackFiles[i];
      job.progress.current = techStackFile.path;
      
      try {
        const processedTechStack = await processTechStackWithLLM(techStackFile, processedProjectFilesWithLLM, llmClient);
        job.processedFiles.push(processedTechStack);
        job.progress.completed++;
      } catch (error) {
        job.errors.push({
          file: techStackFile.path,
          error: error.message,
          timestamp: new Date().toISOString()
        });
        job.progress.failed++;
        
        // Continue processing other tech stack files even if one fails
        console.error(`Error processing tech stack file ${techStackFile.path}:`, error.message);
      }
      
      // Small delay to prevent overwhelming the API
      await new Promise(resolve => setTimeout(resolve, 1000)); // Longer delay for complex processing
    }
    
    // Save processed tech stack files
    if (job.processedFiles.length > 0) {
      const exportResult = await saveProcessedTechStacks(job.projectId, jobId, job.processedFiles, processedProjectFilesWithLLM);
      job.exportPath = exportResult.exportDir;
      job.traceabilityReport = exportResult.traceabilityReport;
    }
    
    // Update final status
    job.status = job.progress.failed === job.progress.total ? JOB_STATUS.FAILED : JOB_STATUS.COMPLETED;
    job.completedAt = new Date().toISOString();
    job.progress.current = null;
    
  } catch (error) {
    job.status = JOB_STATUS.FAILED;
    job.error = error.message;
    job.completedAt = new Date().toISOString();
    console.error(`Job ${jobId} failed:`, error);
  }
};

/**
 * Get processing status
 */
const getStatus = async (req, res) => {
  try {
    const { jobId } = req.params;
    const job = processingJobs.get(jobId);
    
    if (!job) {
      return res.status(404).json({ error: 'Job not found' });
    }
    
    // Calculate progress percentage
    const progressPercentage = job.progress.total > 0 
      ? Math.round(((job.progress.completed + job.progress.failed) / job.progress.total) * 100)
      : 0;
    
    res.json({
      jobId: job.id,
      projectId: job.projectId,
      status: job.status,
      progress: {
        ...job.progress,
        percentage: progressPercentage
      },
      createdAt: job.createdAt,
      startedAt: job.startedAt,
      completedAt: job.completedAt,
      error: job.error,
      errors: job.errors
    });
  } catch (error) {
    console.error('Error getting job status:', error);
    res.status(500).json({ error: error.message });
  }
};

/**
 * Cancel processing
 */
const cancelProcessing = async (req, res) => {
  try {
    const { jobId } = req.params;
    const job = processingJobs.get(jobId);
    
    if (!job) {
      return res.status(404).json({ error: 'Job not found' });
    }
    
    if (job.status === JOB_STATUS.COMPLETED || job.status === JOB_STATUS.FAILED) {
      return res.status(400).json({ error: 'Cannot cancel completed or failed job' });
    }
    
    job.status = JOB_STATUS.CANCELLED;
    job.cancelledAt = new Date().toISOString();
    
    res.json({
      jobId: job.id,
      status: job.status,
      message: 'Processing cancelled'
    });
  } catch (error) {
    console.error('Error cancelling job:', error);
    res.status(500).json({ error: error.message });
  }
};

/**
 * Retry failed processing from specific step
 */
const retryProcessing = async (req, res) => {
  try {
    const { jobId } = req.params;
    const job = processingJobs.get(jobId);
    
    if (!job) {
      return res.status(404).json({ error: 'Job not found' });
    }
    
    if (job.status !== JOB_STATUS.FAILED) {
      return res.status(400).json({ error: 'Can only retry failed jobs' });
    }
    
    // Reset job for retry
    job.status = JOB_STATUS.PENDING;
    job.error = null;
    job.errors = [];
    job.progress.completed = job.processedFiles.length;
    job.progress.failed = 0;
    job.retryCount = (job.retryCount || 0) + 1;
    
    // Start processing from where it left off
    processProjectAsync(jobId);
    
    res.json({
      jobId: job.id,
      status: job.status,
      message: 'Tech stack processing restarted',
      retryCount: job.retryCount
    });
  } catch (error) {
    console.error('Error retrying job:', error);
    res.status(500).json({ error: error.message });
  }
};

/**
 * Get processed tech stack files and results
 */
const getResults = async (req, res) => {
  try {
    const { jobId } = req.params;
    const job = processingJobs.get(jobId);
    
    if (!job) {
      return res.status(404).json({ error: 'Job not found' });
    }
    
    if (job.status !== JOB_STATUS.COMPLETED) {
      return res.status(400).json({ error: 'Job not completed yet' });
    }
    
    res.json({
      jobId: job.id,
      projectId: job.projectId,
      status: job.status,
      processingType: 'tech-stack-update',
      traceabilityReport: job.traceabilityReport,
      exportPath: job.exportPath,
      processedTechStacks: job.processedFiles.map(pts => ({
        techStackId: pts.originalFile.id,
        fileName: pts.originalFile.path,
        appliedGuidelines: pts.processedResult.appliedGuidelines || [],
        unappliedGuidelines: pts.processedResult.unappliedGuidelines || [],
        processingTime: pts.processingTime
      })),
      projectContext: {
        totalProjectFiles: job.projectFiles?.length || 0,
        totalTechStackFiles: job.techStackFiles?.length || 0
      },
      compliance: {
        projectCompliance: job.traceabilityReport?.projectCompliance || '0%',
        totalGuidelines: job.traceabilityReport?.totalGuidelines || 0,
        totalAppliedGuidelines: job.traceabilityReport?.totalAppliedGuidelines || 0,
        totalUnappliedGuidelines: job.traceabilityReport?.totalUnappliedGuidelines || 0
      }
    });
  } catch (error) {
    console.error('Error getting job results:', error);
    res.status(500).json({ error: error.message });
  }
};

/**
 * Download AI agent config zip file
 */
const downloadConfig = async (req, res) => {
  try {
    const { jobId } = req.params;
    const job = processingJobs.get(jobId);
    
    if (!job) {
      return res.status(404).json({ error: 'Job not found' });
    }
    
    if (job.status !== JOB_STATUS.COMPLETED) {
      return res.status(400).json({ error: 'Job not completed yet' });
    }
    
    if (!job.exportPath) {
      return res.status(404).json({ error: 'Export path not found' });
    }
    
    const zipFilePath = path.join(job.exportPath, 'ai-agent-config.zip');
    
    // Check if zip file exists
    try {
      await fs.access(zipFilePath);
    } catch (error) {
      return res.status(404).json({ error: 'AI agent config zip file not found' });
    }
    
    // Set appropriate headers for file download
    res.setHeader('Content-Type', 'application/zip');
    res.setHeader('Content-Disposition', `attachment; filename="ai-agent-config-${jobId}.zip"`);
    
    // Stream the file to the response
    const fileStream = require('fs').createReadStream(zipFilePath);
    fileStream.pipe(res);
    
    fileStream.on('error', (error) => {
      console.error('Error streaming zip file:', error);
      if (!res.headersSent) {
        res.status(500).json({ error: 'Failed to download zip file' });
      }
    });
    
  } catch (error) {
    console.error('Error downloading config:', error);
    res.status(500).json({ error: error.message });
  }
};

module.exports = {
  startProcessing,
  getStatus,
  cancelProcessing,
  retryProcessing,
  getResults,
  downloadConfig
};
